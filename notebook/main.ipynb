{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import random\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy.matlib\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_columns', 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_waps(df):\n",
    "    var1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n",
    "    var2 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n",
    "    var3 = df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']\n",
    "    var4 = df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']\n",
    "    volumes = df['bid_size1'] + df['ask_size1'] + df['bid_size2'] + df['ask_size2']\n",
    "    df['wap1'] = var1 / (df['bid_size1'] + df['ask_size1'])\n",
    "    df['wap2'] = var2 / (df['bid_size2'] + df['ask_size2'])\n",
    "    df['wap3'] = var3 / (df['bid_size1'] + df['ask_size1'])\n",
    "    df['wap4'] = var4 / (df['bid_size2'] + df['ask_size2'])\n",
    "    df['wap12'] = (var1 + var2) / volumes\n",
    "    df['wap34'] = (var3 + var4) / volumes\n",
    "    # Calculate wap balance\n",
    "    df['wap_balance1'] = abs(df['wap1'] - df['wap2'])\n",
    "    df['wap_balance2'] = abs(df['wap3'] - df['wap4'])\n",
    "    return df\n",
    "\n",
    "def calc_log_returns(df):\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n",
    "    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n",
    "    df['log_return12'] = df.groupby(['time_id'])['wap12'].apply(log_return)\n",
    "    df['log_return34'] = df.groupby(['time_id'])['wap34'].apply(log_return)\n",
    "    return df\n",
    "\n",
    "def calc_depth(df):\n",
    "    df['depth'] = df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1'] + df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']\n",
    "    return df\n",
    "\n",
    "def calc_slope(df):\n",
    "    v0 = (df['bid_size1']+df['ask_size1'])/2\n",
    "    p0 = (df['bid_price1']+df['ask_price1'])/2\n",
    "    slope_bid = ((df['bid_size1']/v0)-1)/abs((df['bid_price1']/p0)-1)+(\n",
    "                (df['bid_size2']/df['bid_size1'])-1)/abs((df['bid_price2']/df['bid_price1'])-1)\n",
    "    slope_ask = ((df['ask_size1']/v0)-1)/abs((df['ask_price1']/p0)-1)+(\n",
    "                (df['ask_size2']/df['ask_size1'])-1)/abs((df['ask_price2']/df['ask_price1'])-1)\n",
    "    df['slope_mid'] = (slope_bid + slope_ask) / 2\n",
    "    df['slope_spread'] = abs(slope_bid - slope_ask)\n",
    "    return df\n",
    "\n",
    "def calc_spread(df):    \n",
    "    # Calculate spread\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    # Calculate mid\n",
    "    df['bid_mid1'] = (df['bid_price1'] + df['ask_price1'])/2 - df['bid_price1']\n",
    "    df['bid_mid2'] = (df['bid_price2'] + df['ask_price2'])/2 - df['bid_price2']\n",
    "    df['ask_mid1'] = df['ask_price1'] - (df['bid_price1'] + df['ask_price1'])/2\n",
    "    df['ask_mid2'] = df['ask_price2'] - (df['bid_price2'] + df['ask_price2'])/2\n",
    "    # Calculate dispersion\n",
    "    df['price_spread1'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    \n",
    "    df['bid_ask_spread'] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    return df\n",
    "\n",
    "def calc_price_impact(df):\n",
    "    ask = (df['ask_price1'] * df['ask_size1'] + df['ask_price2'] * df['ask_size2'])/(df['ask_size1']+df['ask_size2'])\n",
    "    bid = (df['bid_price1'] * df['bid_size1'] + df['bid_price2'] * df['bid_size2'])/(df['bid_size1']+df['bid_size2'])\n",
    "    df['bid_impact1'] = (df['bid_price1'] - bid)/df['bid_price1']\n",
    "    df['bid_impact2'] = (df['bid_price2'] - bid)/df['bid_price2']\n",
    "    df['ask_impact1'] = (df['ask_price1'] - ask)/df['ask_price1']\n",
    "    df['ask_impact2'] = (df['ask_price2'] - ask)/df['ask_price2']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_return(wap):\n",
    "    return np.log(wap).diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "def realized_absvar(series):\n",
    "    return np.sqrt(np.pi/(2*series.count()))*np.sum(np.abs(series))\n",
    "\n",
    "def realized_skew(series):\n",
    "    return np.sqrt(series.count())*np.sum(series**3)/(realized_volatility(series)**3)\n",
    "\n",
    "def realized_kurtosis(series):\n",
    "    return series.count()*np.sum(series**4)/(realized_volatility(series)**4)\n",
    "\n",
    "def realized_quarticity(series):\n",
    "    return (series.count()/3)*np.sum(series**4)\n",
    "\n",
    "def realized_skew(series):\n",
    "    return np.sqrt(series.count())*np.sum(series**3)/(realized_volatility(series)**3)\n",
    "\n",
    "def realized_kurtosis(series):\n",
    "    return series.count()*np.sum(series**4)/(realized_volatility(series)**4)\n",
    "\n",
    "def realized_quarticity(series):\n",
    "    return (series.count()/3)*np.sum(series**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique(series):\n",
    "    return len(np.unique(series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read our base train and test set\n",
    "def read_train_test():\n",
    "    train = pd.read_csv(config.data_dir + 'train.csv')\n",
    "    test = pd.read_csv(config.data_dir + 'test.csv')\n",
    "    # Create a key to merge with book and trade data\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    print(f'Our training set has {train.shape[0]} rows')\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    seed = 123\n",
    "    data_dir = '/data/'\n",
    "    time_gap = 100\n",
    "    \n",
    "    feature_dict_book = {\n",
    "        'wap1': [np.sum, np.mean, np.std],\n",
    "        'wap2': [np.sum, np.mean, np.std],\n",
    "        'wap3': [np.sum, np.mean, np.std],\n",
    "        'wap4': [np.sum, np.mean, np.std],\n",
    "        'wap12': [np.sum, np.mean, np.std],\n",
    "        'wap34': [np.sum, np.mean, np.std],\n",
    "        'wap_balance1': [np.sum, np.mean, np.std],\n",
    "        'wap_balance2': [np.sum, np.mean, np.std],\n",
    "        'log_return1': [np.sum, realized_volatility, realized_absvar, realized_skew, np.mean, np.std],\n",
    "        'log_return2': [np.sum, realized_volatility, realized_absvar, realized_skew, np.mean, np.std],\n",
    "        'log_return3': [np.sum, realized_volatility, realized_absvar, realized_skew, np.mean, np.std],\n",
    "        'log_return4': [np.sum, realized_volatility, realized_absvar, realized_skew, np.mean, np.std],\n",
    "        'log_return12': [np.sum, realized_volatility, realized_absvar, realized_skew, np.mean, np.std],\n",
    "        'log_return34': [np.sum, realized_volatility, realized_absvar, realized_skew, np.mean, np.std],\n",
    "        'depth': [np.sum, np.mean, np.std],\n",
    "        'slope_mid': [np.sum, np.mean, np.std],\n",
    "        'slope_spread': [np.sum, np.mean, np.std],\n",
    "        'bid_spread':[np.sum, np.mean, np.std],\n",
    "        'ask_spread':[np.sum, np.mean, np.std],\n",
    "        'bid_mid1':[np.sum, np.mean, np.std],\n",
    "        'bid_mid2':[np.sum, np.mean, np.std],\n",
    "        'ask_mid1':[np.sum, np.mean, np.std],\n",
    "        'ask_mid2':[np.sum, np.mean, np.std],\n",
    "        'price_spread1':[np.sum, np.mean, np.std],\n",
    "        'price_spread2':[np.sum, np.mean, np.std],\n",
    "        'total_volume':[np.sum, np.mean, np.std],\n",
    "        'volume_imbalance':[np.sum, np.mean, np.std],\n",
    "        'bid_ask_spread':[np.sum, np.mean, np.std],\n",
    "        'bid_impact1':[np.sum, np.mean, np.std],\n",
    "        'bid_impact2':[np.sum, np.mean, np.std],\n",
    "        'ask_impact1':[np.sum, np.mean, np.std],\n",
    "        'ask_impact2':[np.sum, np.mean, np.std],\n",
    "    }\n",
    "    \n",
    "    feature_dict_trade = {\n",
    "        'log_return':[realized_volatility, realized_absvar, realized_skew],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, realized_volatility, realized_absvar, realized_skew, np.mean, np.std, np.max, np.min],\n",
    "        'order_count':[np.mean,np.sum,np.max],\n",
    "    }\n",
    "    \n",
    "    model_params = {\n",
    "        \"xgb_bl\": {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"booster\": \"gblinear\",\n",
    "            \"nthread\": -1,\n",
    "            \"eta\": 0.3,\n",
    "            \"max_depth\": 8,\n",
    "            \"min_child_weight\": 1,\n",
    "            \"sampling_method\": \"gradient_based\",\n",
    "            \"tree_method\": \"gpu_hist\"  # turn it on for GPU\n",
    "        },\n",
    "        \"xgb_tuning\": {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"booster\": \"gbtree\",\n",
    "            \"nthread\": -1,\n",
    "            \"tree_method\": \"gpu_hist\",\n",
    "            'max_depth': 7,\n",
    "            'eta': 0.03,\n",
    "            'lambda': 0.01,\n",
    "            \"subsample\": 0.2,\n",
    "            \"colsample_bytree\": 0.33,\n",
    "            \"sampling_method\": \"uniform\"\n",
    "        },\n",
    "        \"xgb_optuna\": {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"booster\": \"gblinear\",\n",
    "            \"nthread\": -1,\n",
    "            \"tree_method\": \"gpu_hist\",\n",
    "            'max_depth': 7,\n",
    "            'eta': 0.03,\n",
    "            'lambda': 1.0979256871605507e-06,\n",
    "            'gamma': 2.3321112461277414e-08,\n",
    "            'alpha': 0.006405029944559645,\n",
    "            \"sampling_method\": \"gradient_based\"\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def book_preprocessor(file_path):\n",
    "    # Function to preprocess book data (for each stock id)\n",
    "    \n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    df = (\n",
    "        df.pipe(calc_waps)\n",
    "        .pipe(calc_log_returns)\n",
    "        .pipe(calc_depth)\n",
    "        .pipe(calc_slope)\n",
    "        .pipe(calc_spread)\n",
    "        .pipe(calc_price_impact)\n",
    "    )\n",
    "    \n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
    "        # Function to get group stats for different windows (seconds in bucket)\n",
    "        \n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(config.feature_dict_book).reset_index()\n",
    "        \n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        \n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "            df_feature.rename({f'time_id__{seconds_in_bucket}': 'time_id_'}, axis=1, inplace=True)\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_list = [get_stats_window(seconds_in_bucket = 0, add_suffix = False)]\n",
    "    \n",
    "    time_slices = [t * config.time_gap for t in range(1, 600 // config.time_gap)]\n",
    "    for t in time_slices:\n",
    "        df_list += [get_stats_window(seconds_in_bucket = t, add_suffix = True)]\n",
    "    \n",
    "    df_feature = reduce(lambda left, right: pd.merge(left, right, on='time_id_'), df_list)\n",
    "    \n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature.loc[:, 'row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    \n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of Trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trade_preprocessor(file_path):\n",
    "    # Function to preprocess trade data (for each stock id)\n",
    "    \n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    \n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
    "        # Function to get group stats for different windows (seconds in bucket)\n",
    "        \n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(config.feature_dict_trade).reset_index()\n",
    "        \n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        \n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "            df_feature.rename({f'time_id__{seconds_in_bucket}': 'time_id_'}, axis=1, inplace=True)\n",
    "        return df_feature\n",
    "    \n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    \n",
    "    def process_trade_features(df):\n",
    "        lis = []\n",
    "        for n_time_id in df['time_id'].unique():\n",
    "            df_id = df[df['time_id'] == n_time_id]        \n",
    "            tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "            f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "            f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "            df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "            df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "            abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "            energy = np.mean(df_id['price'].values**2)\n",
    "            iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "            abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "            energy_v = np.sum(df_id['size'].values**2)\n",
    "            iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "\n",
    "            lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                       'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "\n",
    "        df_lr = pd.DataFrame(lis)\n",
    "        return df_lr\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_list = [get_stats_window(seconds_in_bucket = 0, add_suffix = False)]\n",
    "    \n",
    "    time_slices = [t * 100 for t in range(1, 600 // config.time_gap)]\n",
    "    for t in time_slices:\n",
    "        df_list += [get_stats_window(seconds_in_bucket = t, add_suffix = True)]\n",
    "        \n",
    "    df_feature = reduce(lambda left, right: pd.merge(left, right, on='time_id_'), df_list)\n",
    "    df_lr = process_trade_features(df)\n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    \n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess of Time Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get group stats for the stock_id and time_id\n",
    "def get_time_stock(df):\n",
    "    # Get realized volatility columns\n",
    "    vol_cols = [col for col in df.columns if 'realized_volatility' in col]\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = config.data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = config.data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = config.data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = config.data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, xgb_train):\n",
    "    y_true = xgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_group_k_fold(X, y, groups, k, seed=None):\n",
    "    \"\"\" https://www.kaggle.com/jakubwasikowski/stratified-group-k-fold-cross-validation \"\"\"\n",
    "    labels_num = np.max(y) + 1\n",
    "    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n",
    "    y_distr = Counter()\n",
    "    for label, g in zip(y, groups):\n",
    "        y_counts_per_group[g][label] += 1\n",
    "        y_distr[label] += 1\n",
    "\n",
    "    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n",
    "    groups_per_fold = defaultdict(set)\n",
    "\n",
    "    def eval_y_counts_per_fold(y_counts, fold):\n",
    "        y_counts_per_fold[fold] += y_counts\n",
    "        std_per_label = []\n",
    "        for label in range(labels_num):\n",
    "            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)])\n",
    "            std_per_label.append(label_std)\n",
    "        y_counts_per_fold[fold] -= y_counts\n",
    "        return np.mean(std_per_label)\n",
    "    \n",
    "    groups_and_y_counts = list(y_counts_per_group.items())\n",
    "    random.Random(seed).shuffle(groups_and_y_counts)\n",
    "\n",
    "    for g, y_counts in tqdm(sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])), total=len(groups_and_y_counts)):\n",
    "        best_fold = None\n",
    "        min_eval = None\n",
    "        for i in range(k):\n",
    "            fold_eval = eval_y_counts_per_fold(y_counts, i)\n",
    "            if min_eval is None or fold_eval < min_eval:\n",
    "                min_eval = fold_eval\n",
    "                best_fold = i\n",
    "        y_counts_per_fold[best_fold] += y_counts\n",
    "        groups_per_fold[best_fold].add(g)\n",
    "\n",
    "    all_groups = set(groups)\n",
    "    for i in range(k):\n",
    "        train_groups = all_groups - groups_per_fold[i]\n",
    "        test_groups = groups_per_fold[i]\n",
    "\n",
    "        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n",
    "        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n",
    "\n",
    "        yield train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(train, test):\n",
    "    params = config.model_params['xgb_tuning']\n",
    "    \n",
    "    # Split features and target\n",
    "    x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n",
    "    y = train['target']\n",
    "    x_test = test.drop(['row_id', 'time_id'], axis = 1)\n",
    "    # Transform stock id to a numeric value\n",
    "    feats_nostock = [col for col in x.columns if col not in {\"stock_id\"}] \n",
    "    x['stock_id'] = x['stock_id'].astype(int)\n",
    "    x_test['stock_id'] = x_test['stock_id'].astype(int)\n",
    "    dtest = xgb.DMatrix(x_test)\n",
    "    \n",
    "    # Create out of folds array\n",
    "    oof_predictions = np.zeros(x.shape[0])\n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(x_test.shape[0])\n",
    "    # Create a KFold object\n",
    "    skf = stratified_group_k_fold(\n",
    "        X=x[feats_nostock], y=train['stock_id'].astype('category').cat.codes.values, \n",
    "        groups=np.array(train['time_id'].astype('category').cat.codes.values), k=5, seed=config.seed\n",
    "    )\n",
    "    # Iterate through each fold\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, y_train = x.iloc[train_idx], y.iloc[train_idx]\n",
    "        x_val, y_val = x.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "        x_train[\"stock_id\"] = x_train[\"stock_id\"].astype(int)\n",
    "        x_val[\"stock_id\"] = x_val[\"stock_id\"].astype(int)\n",
    "\n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train, weight=1/np.square(y_train), enable_categorical=True)\n",
    "        dval = xgb.DMatrix(x_val, label=y_val, weight=1/np.square(y_val), enable_categorical=True)\n",
    "\n",
    "        model = xgb.train(params,\n",
    "                          dtrain=dtrain,\n",
    "                          evals=[(dtrain, \"dtrain\"), (dval, \"dval\")],\n",
    "                          verbose_eval=50,\n",
    "                          early_stopping_rounds=100,\n",
    "                          num_boost_round=1000,\n",
    "                          feval=feval_rmspe)\n",
    "\n",
    "        oof_predictions[val_idx] = model.predict(dval)\n",
    "        test_predictions += model.predict(dtest) / 5\n",
    "        \n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
    "    # Return test predictions\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 428932 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed: 33.8min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "# Read train and test\n",
    "train, test = read_train_test()\n",
    "\n",
    "# Get unique stock ids \n",
    "train_stock_ids = train['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "train_ = preprocessor(train_stock_ids, is_train = True)\n",
    "train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get group stats of time_id and stock_id\n",
    "train = get_time_stock(train)\n",
    "test = get_time_stock(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, train=train):\n",
    "    x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n",
    "    y = train['target']\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.25, random_state=config.seed)\n",
    "    dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "    dvalid = xgb.DMatrix(valid_x, label=valid_y)\n",
    "    \n",
    "    param = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 9),\n",
    "        \"eta\": trial.suggest_categorical(\"eta\", [.3, .1]),\n",
    "        \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "        \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-8, 1.0),\n",
    "        \"tree_method\": \"gpu_hist\"  # turn it on for GPU\n",
    "    }\n",
    "\n",
    "    # Add a callback for pruning.\n",
    "    model = xgb.train(param,\n",
    "                      dtrain,\n",
    "                      evals=[(dtrain, \"dtrain\"), (dvalid, \"dval\")],\n",
    "                      verbose_eval=50,\n",
    "                      feval=feval_rmspe,\n",
    "                      num_boost_round=1000,\n",
    "                      early_stopping_rounds=100)\n",
    "    y_pred = model.predict(dvalid)\n",
    "    return rmspe(valid_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = optuna.create_study(\n",
    "#     pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction=\"minimize\"\n",
    "# )\n",
    "# study.optimize(objective, n_trials=100)\n",
    "# print(study.best_trial)\n",
    "\n",
    "# print(study.best_trial)\n",
    "\n",
    "# study.best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp1 - Optuna Best Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Traing and evaluate\n",
    "# test_predictions = train_and_evaluate(train, test)\n",
    "# # Save test predictions\n",
    "# test['target'] = test_predictions\n",
    "# test[['row_id', 'target']].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp2 - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3830/3830 [00:55<00:00, 69.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "[0]\tdtrain-rmse:0.48326\tdtrain-RMSPE:224.11708\tdval-rmse:0.48324\tdval-RMSPE:221.80678\n",
      "[50]\tdtrain-rmse:0.10539\tdtrain-RMSPE:48.87768\tdval-rmse:0.10540\tdval-RMSPE:48.37676\n",
      "[100]\tdtrain-rmse:0.02300\tdtrain-RMSPE:10.66499\tdval-rmse:0.02301\tdval-RMSPE:10.56178\n",
      "[150]\tdtrain-rmse:0.00504\tdtrain-RMSPE:2.33911\tdval-rmse:0.00507\tdval-RMSPE:2.32473\n",
      "[200]\tdtrain-rmse:0.00119\tdtrain-RMSPE:0.55189\tdval-rmse:0.00123\tdval-RMSPE:0.56309\n",
      "[250]\tdtrain-rmse:0.00051\tdtrain-RMSPE:0.23538\tdval-rmse:0.00056\tdval-RMSPE:0.25839\n",
      "[300]\tdtrain-rmse:0.00044\tdtrain-RMSPE:0.20485\tdval-rmse:0.00050\tdval-RMSPE:0.22933\n",
      "[350]\tdtrain-rmse:0.00043\tdtrain-RMSPE:0.20044\tdval-rmse:0.00049\tdval-RMSPE:0.22718\n",
      "[400]\tdtrain-rmse:0.00043\tdtrain-RMSPE:0.19818\tdval-rmse:0.00049\tdval-RMSPE:0.22653\n",
      "[450]\tdtrain-rmse:0.00042\tdtrain-RMSPE:0.19646\tdval-rmse:0.00049\tdval-RMSPE:0.22631\n",
      "[500]\tdtrain-rmse:0.00042\tdtrain-RMSPE:0.19463\tdval-rmse:0.00049\tdval-RMSPE:0.22651\n",
      "[550]\tdtrain-rmse:0.00042\tdtrain-RMSPE:0.19260\tdval-rmse:0.00049\tdval-RMSPE:0.22667\n",
      "[581]\tdtrain-rmse:0.00041\tdtrain-RMSPE:0.19159\tdval-rmse:0.00049\tdval-RMSPE:0.22661\n",
      "Training fold 2\n",
      "[0]\tdtrain-rmse:0.48325\tdtrain-RMSPE:223.67703\tdval-rmse:0.48326\tdval-RMSPE:223.57600\n",
      "[50]\tdtrain-rmse:0.10539\tdtrain-RMSPE:48.78091\tdval-rmse:0.10540\tdval-RMSPE:48.76127\n",
      "[100]\tdtrain-rmse:0.02304\tdtrain-RMSPE:10.66637\tdval-rmse:0.02301\tdval-RMSPE:10.64555\n",
      "[150]\tdtrain-rmse:0.00508\tdtrain-RMSPE:2.35038\tdval-rmse:0.00506\tdval-RMSPE:2.34298\n",
      "[200]\tdtrain-rmse:0.00121\tdtrain-RMSPE:0.56000\tdval-rmse:0.00123\tdval-RMSPE:0.56964\n",
      "[250]\tdtrain-rmse:0.00056\tdtrain-RMSPE:0.25751\tdval-rmse:0.00057\tdval-RMSPE:0.26232\n",
      "[300]\tdtrain-rmse:0.00045\tdtrain-RMSPE:0.20847\tdval-rmse:0.00050\tdval-RMSPE:0.23282\n",
      "[350]\tdtrain-rmse:0.00044\tdtrain-RMSPE:0.20285\tdval-rmse:0.00050\tdval-RMSPE:0.23088\n",
      "[400]\tdtrain-rmse:0.00044\tdtrain-RMSPE:0.20130\tdval-rmse:0.00050\tdval-RMSPE:0.23055\n",
      "[450]\tdtrain-rmse:0.00043\tdtrain-RMSPE:0.19947\tdval-rmse:0.00051\tdval-RMSPE:0.23420\n",
      "[483]\tdtrain-rmse:0.00043\tdtrain-RMSPE:0.19847\tdval-rmse:0.00051\tdval-RMSPE:0.23426\n",
      "Training fold 3\n",
      "[0]\tdtrain-rmse:0.48325\tdtrain-RMSPE:223.54974\tdval-rmse:0.48326\tdval-RMSPE:224.08466\n"
     ]
    }
   ],
   "source": [
    "# Traing and evaluate\n",
    "test_predictions = train_and_evaluate(train, test)\n",
    "# Save test predictions\n",
    "submit_df = test.copy()\n",
    "submit_df['target'] = test_predictions\n",
    "submit_df[['row_id', 'target']].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "volatility",
   "language": "python",
   "name": "volatility"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
