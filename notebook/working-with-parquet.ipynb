{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"[Apache Parquet](https://arrow.apache.org/docs/python/parquet.html) is an efficient columnar storage format. Compared to saving this dataset in csvs using parquet:\n- Greatly reduces the necessary disk space\n- Loads the data into Pandas with memory efficient datatypes\n- Enables fast reads from disk\n- Allows us to easily work with partitions of the data\n\nPandas has a parquet integration that makes loading data into a dataframe trivial; we'll try that now.","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-09T20:36:44.380061Z","iopub.execute_input":"2021-06-09T20:36:44.380655Z","iopub.status.idle":"2021-06-09T20:36:44.389802Z","shell.execute_reply.started":"2021-06-09T20:36:44.380567Z","shell.execute_reply":"2021-06-09T20:36:44.389012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book_train = pd.read_parquet('../input/optiver-realized-volatility-prediction/book_train.parquet')","metadata":{"execution":{"iopub.status.busy":"2021-06-09T20:36:45.299103Z","iopub.execute_input":"2021-06-09T20:36:45.299696Z","iopub.status.idle":"2021-06-09T20:37:02.715087Z","shell.execute_reply.started":"2021-06-09T20:36:45.299663Z","shell.execute_reply":"2021-06-09T20:37:02.713959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If this data were stored as a csv, the numeric types would all default to the 64 bit versions. Parquet retains the more efficient types I specified while saving the data.\n\n**Expect memory usage to spike to roughly double the final dataframe size while parquet loads a file. Consider loading your largest dataset first or using partitions to mitigate this.**","metadata":{}},{"cell_type":"code","source":"book_train.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T20:37:02.716481Z","iopub.execute_input":"2021-06-09T20:37:02.716756Z","iopub.status.idle":"2021-06-09T20:37:02.744022Z","shell.execute_reply.started":"2021-06-09T20:37:02.716728Z","shell.execute_reply":"2021-06-09T20:37:02.742823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The one exception is the `stock_id` column, which has been converted to the category type as it is [the partition column](https://arrow.apache.org/docs/python/parquet.html#reading-from-partitioned-datasets). The parquet files in this dataset are all paritioned by `stock_id` so that it's not necessary to load the entire file at once. In fact, if you examine the parquet files you'll see that they are actually directories.","metadata":{}},{"cell_type":"code","source":"! ls ../input/optiver-realized-volatility-prediction/book_train.parquet | head -n 5","metadata":{"execution":{"iopub.status.busy":"2021-06-09T20:37:02.746026Z","iopub.execute_input":"2021-06-09T20:37:02.746372Z","iopub.status.idle":"2021-06-09T20:37:03.825463Z","shell.execute_reply.started":"2021-06-09T20:37:02.746339Z","shell.execute_reply":"2021-06-09T20:37:03.823845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Those are in turn also directories, which would be relevant if the data were partitioned by more than one column.","metadata":{"execution":{"iopub.status.busy":"2021-06-09T20:05:53.346473Z","iopub.execute_input":"2021-06-09T20:05:53.346883Z","iopub.status.idle":"2021-06-09T20:05:53.353962Z","shell.execute_reply.started":"2021-06-09T20:05:53.346851Z","shell.execute_reply":"2021-06-09T20:05:53.352397Z"}}},{"cell_type":"code","source":"! ls ../input/optiver-realized-volatility-prediction/book_train.parquet/stock_id=0/","metadata":{"execution":{"iopub.status.busy":"2021-06-09T20:37:03.827643Z","iopub.execute_input":"2021-06-09T20:37:03.828055Z","iopub.status.idle":"2021-06-09T20:37:04.930669Z","shell.execute_reply.started":"2021-06-09T20:37:03.82801Z","shell.execute_reply":"2021-06-09T20:37:04.929425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book_train_0 = pd.read_parquet('../input/optiver-realized-volatility-prediction/book_train.parquet/stock_id=0/c439ef22282f412ba39e9137a3fdabac.parquet')\nbook_train_0.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T20:37:41.034482Z","iopub.execute_input":"2021-06-09T20:37:41.034837Z","iopub.status.idle":"2021-06-09T20:37:41.079499Z","shell.execute_reply.started":"2021-06-09T20:37:41.034806Z","shell.execute_reply":"2021-06-09T20:37:41.078284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that because we loaded a single partition, **the partition column was not included**. We could remedy that manually if we need the stock ID or just load a larger subset of the data by passing a list of paths. This will load all of the stock IDs 110-119, reducing memory usesage without implicitly dropping the partition column:","metadata":{"execution":{"iopub.status.busy":"2021-06-09T20:23:35.871533Z","iopub.execute_input":"2021-06-09T20:23:35.872424Z","iopub.status.idle":"2021-06-09T20:23:35.881708Z","shell.execute_reply.started":"2021-06-09T20:23:35.87237Z","shell.execute_reply":"2021-06-09T20:23:35.880548Z"}}},{"cell_type":"code","source":"import glob\nsubset_paths = glob.glob('../input/optiver-realized-volatility-prediction/book_train.parquet/stock_id=11*/*')\nbook_train_subset = pd.read_parquet(subset_paths)\nbook_train_subset.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}